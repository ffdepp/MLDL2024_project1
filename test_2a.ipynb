{"cells":[{"cell_type":"markdown","metadata":{"id":"bH66ar2AGIN7"},"source":["2a"]},{"cell_type":"markdown","metadata":{"id":"8W7MYyHXjQbx"},"source":["Dataset and DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20031,"status":"ok","timestamp":1716903752650,"user":{"displayName":"Tanguy Theiss","userId":"07268874191182726823"},"user_tz":-120},"id":"6LE5D4ibkoFl","outputId":"c017edb4-61d4-4bed-d725-90411bd9cb64"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSZamnRIGMBo"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import time\n","import requests\n","from torchvision.datasets import ImageFolder\n","from torchvision import models\n","from PIL import Image\n","from shutil import copyfile\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ErhRLKwGOYw"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/MLDL_project')\n","from datasets.cityscapes import CityscapesDataset\n","from datasets.gta5 import GTA5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qqY939g2Mdlf"},"outputs":[],"source":["import torchvision.transforms as transforms\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJdnjm9ejQb6"},"outputs":[],"source":["train_dataset = CityscapesDataset('/content/drive/MyDrive/MLDL_project/datasets/Cityspaces', transform=transform, split='train')\n","val_dataset = CityscapesDataset('/content/drive/MyDrive/MLDL_project/datasets/Cityspaces', transform=transform, split='val')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRib7A3cjQb6"},"outputs":[],"source":["# Create a DataLoader\n","from torch.utils.data import DataLoader\n","dataloader_train = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=8)\n","dataloader_val = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1716903764231,"user":{"displayName":"Tanguy Theiss","userId":"07268874191182726823"},"user_tz":-120},"id":"oOAvJ_1AjQb7","outputId":"5ce1b61d-14b7-488e-b7b8-40e122395da1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training samples: 1572\n","Number of validation samples: 500\n"]}],"source":["num_train_samples = len(train_dataset)\n","num_val_samples = len(val_dataset)\n","\n","print(f'Number of training samples: {num_train_samples}')\n","print(f'Number of validation samples: {num_val_samples}')"]},{"cell_type":"markdown","metadata":{"id":"zJfIPq_ajQb8"},"source":["Build model"]},{"cell_type":"code","source":["# Load pre-trained model\n","\n","from models.deeplabv2.deeplabv2 import get_deeplab_v2\n","model = get_deeplab_v2().cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLy0rTKDUUJM","executionInfo":{"status":"ok","timestamp":1716903771676,"user_tz":-120,"elapsed":7463,"user":{"displayName":"Tanguy Theiss","userId":"07268874191182726823"}},"outputId":"64bb61a1-10d2-41fd-d86b-e051b2c2ab77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Deeplab pretraining loading...\n"]}]},{"cell_type":"markdown","metadata":{"id":"o6hJEELV5dy6"},"source":["Trainning and validation process"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24238,"status":"ok","timestamp":1716886293410,"user":{"displayName":"Zepeng Li","userId":"04563086650751250217"},"user_tz":-120},"id":"rBtsmM0fLZse","outputId":"c893b50a-9e8a-4881-f3f5-68f44daa9da4"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 19, 512, 1024]) torch.Size([2, 512, 1024])\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"name":"stdout","output_type":"stream","text":["Avg. Training Loss: 6.983089447021484, mIoU: 0.012453069959973593\n"]}],"source":["# This is just used to test if you can train your model with this dataloader\n","# from utils import poly_lr_scheduler\n","# from utils import fast_hist\n","# from utils import per_class_iou\n","# from utils import total_hist\n","\n","# def total_hist(outputs, labels, num_classes):\n","#     hist = 0\n","#     for i in range(len(outputs)):\n","#         output, label = outputs[i].cpu().detach().numpy().reshape(-1,), labels[i].cpu().detach().numpy().reshape(-1,)\n","#         hist += fast_hist(label, output, num_classes)\n","#     return hist\n","#\n","# model.train()\n","# running_loss = 0.0\n","# hist = 0\n","# criterion = nn.CrossEntropyLoss(ignore_index=255)\n","# optimizer = optim.Adam(model.parameters(), lr=0.0001)  #Since our batch size is only 2, so we need to choose a small initial learning rate\n","\n","# for i, (inputs, labels) in enumerate(dataloader_train, 0):\n","#     inputs, labels = inputs.cuda(), labels.cuda()\n","#     optimizer.zero_grad()\n","\n","#     outputs, _, _ = model(inputs)\n","#     print(outputs.shape, labels.shape)\n","#     loss = criterion(outputs, labels)\n","#     loss.backward()\n","#     optimizer.step()\n","\n","#     running_loss += loss.item()\n","#     outputs = torch.argmax(outputs, dim=1)\n","#     hist += total_hist(outputs, labels, 19)\n","#     break\n","# avg_loss = running_loss\n","# miou = np.mean(per_class_iou(hist))\n","# print(f\"Avg. Training Loss: {avg_loss}, mIoU: {miou}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhsWe6GrYJG2"},"outputs":[],"source":["from train import train\n","from validate import validation"]},{"cell_type":"code","source":["def poly_lr_scheduler(optimizer, init_lr=0.0001, init_lr_cls=0.001, iter=0, lr_decay_iter=1,\n","                      max_iter=300, power=0.9):\n","    \"\"\"Polynomial decay of learning rate\n","            :param init_lr is base learning rate\n","            :param init_lr_cls is base learning rate for final classifier layer\n","            :param iter is a current iteration\n","            :param lr_decay_iter how frequently decay occurs, default is 1\n","            :param max_iter is number of maximum iterations\n","            :param power is a polymomial power\n","\n","    \"\"\"\n","    # if iter % lr_decay_iter or iter > max_iter:\n","    # \treturn optimizer\n","\n","    lr = init_lr*(1 - iter/max_iter)**power\n","    optimizer.param_groups[0]['lr'] = lr\n","\n","    lr_cls = init_lr_cls*(1 - iter/max_iter)**power\n","    optimizer.param_groups[1]['lr'] = lr_cls\n","    return lr, lr_cls"],"metadata":{"id":"Mt0bIm7hvBRU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":491},"id":"KnWIBshs3vX7","outputId":"225330c8-2dd8-499a-bd30-1a4f0334f480","executionInfo":{"status":"error","timestamp":1716905384772,"user_tz":-120,"elapsed":7477,"user":{"displayName":"Tanguy Theiss","userId":"07268874191182726823"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch1 Avg. Training Loss: 0.4060953312171931, mIoU: 0.4208803753802576\n","Avg. Validation Loss: 0.2923396674692631, mIoU: 0.49425985346824197\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-6f847bc1119c>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------------------------------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiou_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiou_per_class_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# curr_lr = poly_lr_scheduler(optimizer = optimizer, init_lr = 0.0001, iter = epoch, lr_decay_iter=1, max_iter=epochs, power=0.9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# every 5 epochs print the miou and loss of validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/MLDL_project/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, dataloader_train, criterion, optimizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mhist\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=255)\n","optimizer = optim.SGD([\n","    {'params': [param for name, param in model.named_parameters() if 'layer6' not in name]},\n","    {'params': model.layer6.parameters(), 'lr': 0.001}\n","], lr=0.0001, momentum=0.9, weight_decay=0.0005)\n","\n","# training model\n","epochs = 50\n","\n","miou_train_list = []\n","miou_val_list = []\n","models = []\n","for epoch in range(epochs):\n","    print('--------------------------------------------------------------------------------')\n","    _, miou_train, miou_per_class_train = train(epoch, model, dataloader_train, criterion, optimizer)\n","    lr, lr_cls = poly_lr_scheduler(optimizer = optimizer, init_lr=0.0001, init_lr_cls=0.001, iter=epoch, lr_decay_iter=1, max_iter=epochs, power=0.9)\n","    # every 5 epochs print the miou and loss of validation set\n","    if epoch % 5 == 0:\n","      _, miou_val, miou_per_class_val = validation(model, dataloader_val, criterion)\n","      print('--------------------------------------------------------------------------------')\n","      miou_train_list.append(miou_train)\n","      miou_val_list.append(miou_val)\n","      models.append(model.state_dict())\n","\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktodfHCQt8lm"},"outputs":[],"source":["# save best model\n","import os\n","DIR = '/content/drive/MyDrive/MLDL_project/models/deeplabv2/trained_models/'\n","if not os.path.exists(DIR):\n","    os.makedirs(DIR)\n","PATH = DIR + f'deepLabV2_epoch{epochs}.pth'\n","\n","# delete old model files\n","if os.path.exists(PATH):\n","    os.remove(PATH)\n","\n","model = get_deeplab_v2().cuda()\n","model.load_state_dict(models[np.argmax(np.array(miou_val_list))])\n","torch.save(model.state_dict(), PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYjcbuGb7DCu"},"outputs":[],"source":["# visualization\n","import matplotlib.pyplot as plt\n","\n","epochs_list = np.arange(1, epochs+1, 5)\n","plt.figure(figsize=(10, 6))\n","\n","plt.plot(epochs_list, miou_train_list, marker='o', linestyle='-', color='r', label='Training mIOU')\n","plt.plot(epochs_list, miou_val_list, marker='o', linestyle='--', color='b', label='Val mIOU')\n","\n","plt.title('Training and validation mIOU over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('mIOU')\n","\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1qHDiFfE2tx2"},"source":["Flops and Number of parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVmFKcxm2-K7"},"outputs":[],"source":["!pip install -U fvcore"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"za6HIEhd2uwc"},"outputs":[],"source":["from fvcore.nn import FlopCountAnalysis, flop_count_table\n","\n","height = train_dataset[0][0].shape[0]\n","width = train_dataset[0][0].shape[1]\n","image = torch.zeros((1, 3, height, width)).cuda()\n","\n","flops = FlopCountAnalysis(model, image)\n","print(flops)\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f'Total number of parameters: {total_params}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwmNMYhZ4FjC"},"outputs":[],"source":["#more detail information about number of parameters and flops\n","print(flop_count_table(flops))"]},{"cell_type":"markdown","metadata":{"id":"xsFTGael3Brh"},"source":["Latency and FPS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bk1qMMly3Awu"},"outputs":[],"source":["# latency and FPS\n","import time\n","\n","height = train_dataset[0][0].shape[0]\n","width = train_dataset[0][0].shape[1]\n","image = np.random.randint(0,256,(height, width, 3)) / 255.\n","image = transform(image)\n","image = torch.unsqueeze(image, dim=0).float().cuda()\n","\n","iterations = 1000\n","latency = np.zeros(iterations)\n","fps = np.zeros(iterations)\n","for i in range(iterations):\n","  start = time.time()\n","  output = model(image)\n","  end = time.time()\n","  time_diff_seconds = end - start\n","  latency[i] = time_diff_seconds\n","  fps[i] = 1/time_diff_seconds\n","\n","meanLatency = np.mean(latency)*1000\n","stdLatency = np.std(latency)*1000\n","meanFPS = np.mean(fps)\n","stdFPS = np.std(fps)\n","\n","print(f\"Mean Latency: {meanLatency} ms\")\n","print(f\"Std Latency: {stdLatency} ms\")\n","print(f\"Mean FPS: {meanFPS}\")\n","print(f\"Std FPS: {stdFPS}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1awP80Qz5doG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}