{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH66ar2AGIN7"
      },
      "source": [
        "3a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W7MYyHXjQbx"
      },
      "source": [
        "Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LE5D4ibkoFl",
        "outputId": "371bd1f4-c555-4b37-8ad3-62f716265aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RSZamnRIGMBo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "import requests\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from shutil import copyfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5ErhRLKwGOYw"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/MLDL_project')\n",
        "from datasets.cityscapes import CityscapesDataset\n",
        "from datasets.gta5 import GTA5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qqY939g2Mdlf"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((720,1280), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "target_transform_train = transforms.Compose([\n",
        "    transforms.Resize((720,1280), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hJdnjm9ejQb6"
      },
      "outputs": [],
      "source": [
        "train_dataset_source = GTA5('/content/drive/MyDrive/MLDL_project/datasets/GTA5', transform=transform_train, target_transform=target_transform_train)\n",
        "train_dataset_target = CityscapesDataset('/content/drive/MyDrive/MLDL_project/datasets/Cityspaces', transform=transform_val, split='train')\n",
        "val_dataset = CityscapesDataset('/content/drive/MyDrive/MLDL_project/datasets/Cityspaces', transform=transform_val, split='val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRib7A3cjQb6",
        "outputId": "cd08fd5b-2662-43bd-ea6e-f653d35af031"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Create a DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader_train_source = DataLoader(train_dataset_source, batch_size=4, shuffle=True, num_workers=8)\n",
        "dataloader_train_target = DataLoader(train_dataset_target, batch_size=4, shuffle=True, num_workers=8)\n",
        "dataloader_val = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOAvJ_1AjQb7",
        "outputId": "b98f9639-ea84-4dd6-b936-480a77ed1fb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of source training samples: 2500\n",
            "Number of target training samples: 1572\n",
            "Number of validation samples: 500\n"
          ]
        }
      ],
      "source": [
        "num_train_samples_source = len(train_dataset_source)\n",
        "num_train_samples_target = len(train_dataset_target)\n",
        "num_val_samples = len(val_dataset)\n",
        "\n",
        "print(f'Number of source training samples: {num_train_samples_source}')\n",
        "print(f'Number of target training samples: {num_train_samples_target}')\n",
        "print(f'Number of validation samples: {num_val_samples}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJfIPq_ajQb8"
      },
      "source": [
        "Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ND0E5w4gHo2z"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model\n",
        "\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "model = BiSeNet(num_classes = 19, context_path='resnet18').cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lEp_VWkw4h8v"
      },
      "outputs": [],
      "source": [
        "# Load discriminator model\n",
        "from models.discriminator.discriminator import FCDiscriminator\n",
        "model_D = FCDiscriminator(num_classes=19).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6hJEELV5dy6"
      },
      "source": [
        "Trainning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgM4Ta8ppJEv"
      },
      "outputs": [],
      "source": [
        "def train(epoch, model, model_D, dataloader_train_source, dataloader_train_target, criterion, criterion_D, optimizer, optimizer):\n",
        "  model.train()\n",
        "  model_D.train()\n",
        "  loss_seg_value = 0.0\n",
        "  loss_adv_target_value = 0.0\n",
        "  loss_D_value = 0.0\n",
        "  hist = 0\n",
        "  for i, (data_source, data_target) in enumerate(itertools.zip_longest(dataloader_train_source, dataloader_train_target, fillvalue=None)):\n",
        "    inputs_s, lables_s = data_source[0].cuda(), data_source[1].cuda()\n",
        "    if data_target is not None:\n",
        "      inputs_t, _ = data_target[0].cuda(), data_target[1].cuda()\n",
        "    optimizer.zero_grad()\n",
        "    optimizer_D.zero_grad()\n",
        "\n",
        "    # train G\n",
        "\n",
        "    # don't accumulate grads in D\n",
        "    for param in model_D.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # train with source\n",
        "    pre_s,_,_ = model(inputs_s)\n",
        "    loss_seg = criterion(pre_s, lables_s)\n",
        "    loss_seg.backward()\n",
        "    loss_seg_value += loss_seg.data.cpu().numpy()[0]\n",
        "\n",
        "    # train with target\n",
        "    if data_target is not None:\n",
        "      pre_t,_,_ = model(inputs_t)\n",
        "      D_out = model_D(F.softmax(pre_t))\n",
        "      loss_adv_target = criterion_D(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label)).cuda())\n",
        "      loss_adv = lambda_adv_target * loss_adv_target\n",
        "      loss_adv.backend()\n",
        "      loss_adv_target_value += loss_adv_target.data.cpu().numpy()[0]\n",
        "\n",
        "    # train D\n",
        "\n",
        "    # bring back requires_grad\n",
        "    for param in model_D.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # train with source\n",
        "    pre_s = pre_s.detach()\n",
        "    D_out = model_D(F.softmax(pre_s))\n",
        "    loss_D = criterion_D(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label)).cuda())\n",
        "    loss_D.backend()\n",
        "    loss_D_value += loss_D.data.cpu().numpy()[0]\n",
        "\n",
        "    # train with target\n",
        "    if data_target is not None:\n",
        "      pre_t = pre_t.detach()\n",
        "      D_out = model_D(F.softmax(pre_t))\n",
        "      loss_D = criterion_D(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(target_label)).cuda())\n",
        "      loss_D.backward()\n",
        "      loss_D_value += loss_D.data.cpu().numpy()[0]\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    _, predicted = pre_s.max(1)\n",
        "    hist += total_hist(predicted, lables_s, 19)\n",
        "\n",
        "  miou_per_class = per_class_iou(hist)\n",
        "  miou = np.mean(miou_per_class)\n",
        "\n",
        "  # pay attention, this is different with AdaptSegNet\n",
        "  loss_seg_value = loss_seg_value / len(dataloader_train_source)\n",
        "  loss_adv_target_value = loss_adv_target_value / len(dataloader_train_target)\n",
        "  loss_D_value = loss_D_value / (len(dataloader_train_source) + len(dataloader_train_target))\n",
        "  print(f\"Epoch{epoch+1}, Loss_seg: {loss_seg_value}, Loss_adv: {loss_adv_target_value}, Loss_D: {loss_D_value}, mIOU: {miou}\")\n",
        "\n",
        "  return loss_seg_value, loss_adv_target_value, loss_D_value, miou, miou_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmxH9G7MzNwj"
      },
      "outputs": [],
      "source": [
        "def validation(model, model_D, dataloader_train_source, dataloader_train_target, criterion, criterion_D, optimizer, optimizer):\n",
        "  # implement here\n",
        "  # ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OBND5btsA9I"
      },
      "outputs": [],
      "source": [
        "from utils import poly_lr_scheduler\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "optimizer = optim.SGD(model.parameters(), lr=(2.5e-2)/4, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "criterion_D = nn.BCEWithLogitsLoss()\n",
        "optimizer_D = optim.Adam(model_D.parameters(), lr=1e-4, betas=(0.9, 0.99))\n",
        "\n",
        "# labels for adversarial training\n",
        "source_label = 0\n",
        "target_label = 1\n",
        "# hyper parameter\n",
        "lambda_adv_target = 0.001\n",
        "\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "  # uisng train and validation here\n",
        "  # ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktodfHCQt8lm"
      },
      "outputs": [],
      "source": [
        "# save best model\n",
        "import os\n",
        "DIR = '/content/drive/MyDrive/MLDL_project/models/bisenet/trained_models/'\n",
        "if not os.path.exists(DIR):\n",
        "    os.makedirs(DIR)\n",
        "PATH = DIR + f'biseNet_UDA_epoch{epochs}.pth'\n",
        "\n",
        "# delete old model files\n",
        "if os.path.exists(PATH):\n",
        "    os.remove(PATH)\n",
        "\n",
        "model = BiSeNet(num_classes = 19, context_path='resnet18').cuda()\n",
        "model.load_state_dict(models[np.argmax(np.array(miou_val_list))])\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYjcbuGb7DCu"
      },
      "outputs": [],
      "source": [
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_list = np.arange(1, epochs+1, 5)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(epochs_list, miou_train_list, marker='o', linestyle='-', color='r', label='Training mIOU')\n",
        "plt.plot(epochs_list, miou_val_list, marker='o', linestyle='--', color='b', label='Val mIOU')\n",
        "\n",
        "plt.title('Training and validation mIOU over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('mIOU')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qHDiFfE2tx2"
      },
      "source": [
        "Flops and Number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVmFKcxm2-K7"
      },
      "outputs": [],
      "source": [
        "!pip install -U fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za6HIEhd2uwc"
      },
      "outputs": [],
      "source": [
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "\n",
        "height = train_dataset[0][0].shape[0]\n",
        "width = train_dataset[0][0].shape[1]\n",
        "image = torch.zeros((1, 3, height, width)).cuda()\n",
        "\n",
        "flops = FlopCountAnalysis(model, image)\n",
        "print(flops)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'Total number of parameters: {total_params}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwmNMYhZ4FjC"
      },
      "outputs": [],
      "source": [
        "#more detail information about number of parameters and flops\n",
        "print(flop_count_table(flops))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsFTGael3Brh"
      },
      "source": [
        "Latency and FPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk1qMMly3Awu"
      },
      "outputs": [],
      "source": [
        "# latency and FPS\n",
        "import time\n",
        "\n",
        "height = train_dataset[0][0].shape[0]\n",
        "width = train_dataset[0][0].shape[1]\n",
        "image = np.random.randint(0,256,(height, width, 3)) / 255.\n",
        "image = transform(image)\n",
        "image = torch.unsqueeze(image, dim=0).float().cuda()\n",
        "\n",
        "iterations = 1000\n",
        "latency = np.zeros(iterations)\n",
        "fps = np.zeros(iterations)\n",
        "for i in range(iterations):\n",
        "  start = time.time()\n",
        "  output = model(image)\n",
        "  end = time.time()\n",
        "  time_diff_seconds = end - start\n",
        "  latency[i] = time_diff_seconds\n",
        "  fps[i] = 1/time_diff_seconds\n",
        "\n",
        "meanLatency = np.mean(latency)*1000\n",
        "stdLatency = np.std(latency)*1000\n",
        "meanFPS = np.mean(fps)\n",
        "stdFPS = np.std(fps)\n",
        "\n",
        "print(f\"Mean Latency: {meanLatency} ms\")\n",
        "print(f\"Std Latency: {stdLatency} ms\")\n",
        "print(f\"Mean FPS: {meanFPS}\")\n",
        "print(f\"Std FPS: {stdFPS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1awP80Qz5doG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
