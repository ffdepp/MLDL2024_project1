{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U fvcore"
      ],
      "metadata": {
        "id": "5L6qQM2yumkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3840a16-a845-48f8-dae4-cdf62dc78e19",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m412.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.25.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.11.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=a4a0950023a1e214c9fdf8438c28bdeab0f805e7b6a7d4717dca9aeace1d81f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=b59a8f4379fd819b9fab84f763eff8a07936d5509d95cceae989f2749e3f3ba0\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV4ErT9wtvUe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "import requests\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from shutil import copyfile\n",
        "import os\n",
        "\n",
        "os.chdir('/content/drive/My Drive')\n",
        "from datasets.cityscapes import CityscapesDataset\n",
        "from datasets.gta5 import GTA5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "5cH4YulCupCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zMRfDx1mRiA0",
        "outputId": "3998049b-ebd5-4d55-894a-909dfab517d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((720,1280), interpolation=Image.Resampling.NEAREST),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_lbl_transform = transforms.Compose([\n",
        "    transforms.Resize((720,1280), interpolation=Image.Resampling.NEAREST),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((512,1024), interpolation=Image.Resampling.NEAREST),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_lbl_transform = transforms.Compose([\n",
        "    transforms.Resize((512,1024), interpolation=Image.Resampling.NEAREST),\n",
        "])"
      ],
      "metadata": {
        "id": "ZA1yBcThvVtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = GTA5('/content/drive/MyDrive/datasets/GTA5', transform=[train_transform,train_lbl_transform]) # datasets\\\\GTA5\n",
        "val_dataset = CityscapesDataset('/content/drive/MyDrive/datasets/Cityscapes/Cityscapes/Cityspaces', transform=[val_transform,val_lbl_transform]) # datasets\\\\Cityscapes\\\\Cityspaces\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers = 8) #16\n",
        "val_dataloader = DataLoader(val_dataset, split='val', batch_size=4, shuffle=True, num_workers = 8) #16"
      ],
      "metadata": {
        "id": "4SBPPnyCvbad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_dataloader:\n",
        "    # Access the first image and label\n",
        "    image, label = images[0], labels[0]\n",
        "    print(\"Image shape:\", image.shape)\n",
        "    print(\"Label shape:\", label.shape)\n",
        "    print(\"Label:\", label)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "Zs0DqvzWvnyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "895bbe0e-9005-4416-c942-336ed79a6493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([3, 720, 1280])\n",
            "Label shape: torch.Size([720, 1280])\n",
            "Label: tensor([[ 10,  10,  10,  ...,   8,   8,   8],\n",
            "        [ 10,  10,  10,  ...,   8,   8,   8],\n",
            "        [ 10,  10,  10,  ...,   8,   8,   8],\n",
            "        ...,\n",
            "        [  0,   0,   0,  ..., 255, 255, 255],\n",
            "        [  0,   0,   0,  ..., 255, 255, 255],\n",
            "        [  0,   0,   0,  ..., 255, 255, 255]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in val_dataloader:\n",
        "    # Access the first image and label\n",
        "    image, label = images[0], labels[0]\n",
        "    print(\"Image shape:\", image.shape)\n",
        "    print(\"Label shape:\", label.shape)\n",
        "    print(\"Label:\", label)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "jnS0pzCIvpsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1733c78-61bc-40de-fb47-155805fafc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([3, 512, 1024])\n",
            "Label shape: torch.Size([512, 1024])\n",
            "Label: tensor([[255, 255, 255,  ..., 255, 255, 255],\n",
            "        [255, 255, 255,  ..., 255, 255, 255],\n",
            "        [255, 255, 255,  ..., 255, 255, 255],\n",
            "        ...,\n",
            "        [255, 255, 255,  ...,   0, 255, 255],\n",
            "        [255, 255, 255,  ..., 255, 255, 255],\n",
            "        [255, 255, 255,  ..., 255, 255, 255]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model\n",
        "\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "bisenet = BiSeNet(num_classes = 19, context_path='resnet18').to(device)"
      ],
      "metadata": {
        "id": "M4lQ4ehNwTLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bisenet"
      ],
      "metadata": {
        "id": "ITVp3quRwVZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7e88ec-10e8-4825-c06f-6e8059ad08d5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiSeNet(\n",
              "  (saptial_path): Spatial_path(\n",
              "    (convblock1): ConvBlock(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (convblock2): ConvBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (convblock3): ConvBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (context_path): resnet18(\n",
              "    (features): ResNet(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): BasicBlock(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (1): BasicBlock(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): BasicBlock(\n",
              "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): BasicBlock(\n",
              "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): BasicBlock(\n",
              "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): BasicBlock(\n",
              "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): BasicBlock(\n",
              "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): BasicBlock(\n",
              "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              "    )\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (attention_refinement_module1): AttentionRefinementModule(\n",
              "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (attention_refinement_module2): AttentionRefinementModule(\n",
              "    (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (supervision1): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (supervision2): Conv2d(512, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (feature_fusion_module): FeatureFusionModule(\n",
              "    (convblock): ConvBlock(\n",
              "      (conv1): Conv2d(1024, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (conv1): Conv2d(19, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (relu): ReLU()\n",
              "    (conv2): Conv2d(19, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (sigmoid): Sigmoid()\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (conv): Conv2d(19, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "kyKAkLwBwXpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the length of class_names (one output unit for each class)\n",
        "output_shape = 19 # 0 to 18\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
        "initial_lr = 2.5e-2/4\n",
        "optimizer = torch.optim.SGD(bisenet.parameters(), lr=initial_lr, momentum=0.9, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "wPNbyo1TwZob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.utils import fast_hist\n",
        "from models.utils import per_class_iou\n",
        "# from utils import fast_hist\n",
        "# from utils import per_class_iou\n",
        "\n",
        "def total_hist(outputs, labels, num_classes):\n",
        "    hist = 0\n",
        "    for i in range(len(outputs)):\n",
        "        output, label = outputs[i].cpu().detach().numpy().reshape(-1,), labels[i].cpu().detach().numpy().reshape(-1,)\n",
        "        hist += fast_hist(label, output, num_classes)\n",
        "    return hist"
      ],
      "metadata": {
        "id": "XTjAtUxyJep1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, dataloader, loss_fn, alpha = 1):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    hist = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        # targets = torch.argmax(targets, dim=1) # remember that this is necessary because the loss function expects differents shapes (batch_size, n_classes, height, width) for pred and (batch_size, height, width) for target\n",
        "\n",
        "        outputs, cx1_sup, cx2_sup = model(inputs)\n",
        "\n",
        "        # main loss\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # # supervision losses\n",
        "        # sup1_loss = loss_fn(cx1_sup, targets)\n",
        "        # sup2_loss = loss_fn(cx2_sup, targets)\n",
        "\n",
        "        # # combined loss\n",
        "        # loss = main_loss + alpha * (sup1_loss + sup2_loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        hist += total_hist(predicted, targets, 19)\n",
        "\n",
        "    train_loss = running_loss / len(dataloader)\n",
        "    miou_per_class = per_class_iou(hist)\n",
        "    miou = np.mean(miou_per_class)\n",
        "\n",
        "    return train_loss, miou, miou_per_class\n",
        "\n",
        "\n",
        "def test(model, dataloader, loss_fn):\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    hist = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            # targets = torch.argmax(targets, dim=1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            hist += total_hist(predicted, targets, 19)\n",
        "\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "\n",
        "    miou_per_class = per_class_iou(hist)\n",
        "    miou = np.mean(miou_per_class)\n",
        "\n",
        "    return test_loss, miou, miou_per_class"
      ],
      "metadata": {
        "id": "ldTyCN1Oy4_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.utils import poly_lr_scheduler\n",
        "# from utils import poly_lr_scheduler"
      ],
      "metadata": {
        "id": "r1R6OVq62gKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "epochs = 5 #50\n",
        "\n",
        "miou_train_list = []\n",
        "miou_test_list = []\n",
        "miou_per_class_train_list = []\n",
        "miou_per_class_test_list = []\n",
        "\n",
        "for ep in range(epochs):\n",
        "\n",
        "    train_loss, train_miou, train_miou_per_class = train(bisenet, optimizer, train_dataloader, loss_fn)\n",
        "    test_loss, test_miou, test_miou_per_class = test(bisenet, val_dataloader, loss_fn)\n",
        "\n",
        "    ## Inserting learning rate decay\n",
        "    curr_lr = poly_lr_scheduler(optimizer = optimizer, init_lr = initial_lr, iter = ep, lr_decay_iter=1, max_iter=epochs, power=0.9)\n",
        "\n",
        "    miou_train_list.append(train_miou)\n",
        "    miou_test_list.append(test_miou)\n",
        "    miou_per_class_train_list.append(train_miou_per_class)\n",
        "    miou_per_class_test_list.append(test_miou_per_class)\n",
        "\n",
        "    print(f\"Current learning rate: {curr_lr}\")\n",
        "    print(f\"Test loss: {test_loss}\")\n",
        "    print(f\"Train mIoU: {train_miou}\")\n",
        "    print(f\"Test mIoU: {test_miou}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "skX4HmzEuppl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4b1255-f16e-4787-f385-825c724842d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current learning rate: 0.00625\n",
            "Test loss: 1.946525250075731\n",
            "Train mIoU: 0.09953300963256177\n",
            "Test mIoU: 0.05214746235555472\n",
            "\n",
            "\n",
            "Current learning rate: 0.005112825912817865\n",
            "Test loss: 1.679348918014507\n",
            "Train mIoU: 0.10372235722472083\n",
            "Test mIoU: 0.056841860290313864\n",
            "\n",
            "\n",
            "Current learning rate: 0.0039465366718084705\n",
            "Test loss: 1.7488345881454816\n",
            "Train mIoU: 0.11000606742996485\n",
            "Test mIoU: 0.055031763311853214\n",
            "\n",
            "\n",
            "Current learning rate: 0.0027398955659630434\n",
            "Test loss: 1.7640762647599664\n",
            "Train mIoU: 0.11555793680537339\n",
            "Test mIoU: 0.06788594054777679\n",
            "\n",
            "\n",
            "Current learning rate: 0.0014682736788600235\n",
            "Test loss: 1.8223636147326792\n",
            "Train mIoU: 0.12182981752714674\n",
            "Test mIoU: 0.07081265606196334\n",
            "\n",
            "\n",
            "[INFO] Total training time: 3099.795 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# latency and FPS\n",
        "\n",
        "import time\n",
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "iterations = 1000\n",
        "random_sampler = RandomSampler(val_dataset, replacement=False, num_samples=iterations)\n",
        "random_dataloader = DataLoader(val_dataset, batch_size=1, sampler=random_sampler)\n",
        "\n",
        "latency = []\n",
        "fps = []\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in random_dataloader:\n",
        "        inputs = inputs.cuda()\n",
        "        start = time.time()\n",
        "        prediction = bisenet(inputs)\n",
        "        end = time.time()\n",
        "        latency_i = end - start\n",
        "        latency.append(latency_i)\n",
        "        fps_i = 1/latency_i\n",
        "        fps.append(fps_i)\n",
        "\n",
        "meanLatency = np.mean(latency)*1000\n",
        "stdLatency = np.std(latency)*1000\n",
        "meanfps = np.mean(fps)\n",
        "stdfps = np.std(fps)\n",
        "\n",
        "print(\"Mean Latency:\", meanLatency)\n",
        "print(\"Std Latency:\", stdLatency)\n",
        "print(\"Mean FPS:\", meanfps)\n",
        "print(\"Std FPS:\", stdfps)"
      ],
      "metadata": {
        "id": "Dtl7bWIfvPa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FLOPs and parameters\n",
        "\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "\n",
        "height = 512\n",
        "width = 1024\n",
        "image = torch.zeros((3, height, width))\n",
        "\n",
        "flops = FlopCountAnalysis(bisenet, image)\n",
        "print(flop_count_table(flops))"
      ],
      "metadata": {
        "id": "Nl1Scp7HvLrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getting_val_miou(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "    hist = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            targets = torch.argmax(targets, dim=1)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            outputs = torch.argmax(outputs, dim=1)\n",
        "            hist += total_hist(outputs, targets, 19)\n",
        "\n",
        "    miou_per_class = per_class_iou(hist)\n",
        "    miou = np.mean(miou_per_class)\n",
        "\n",
        "    return miou, miou_per_class"
      ],
      "metadata": {
        "id": "H-UWKPmRvJDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "miou, miou_per_class = getting_val_miou(bisenet, val_dataloader)\n",
        "\n",
        "print(f\"Average mIoU:{miou}\")\n",
        "print(f\"Average mIoU per class:{miou_per_class}\")"
      ],
      "metadata": {
        "id": "aWT1jZ84u4z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2626bdc4-87d9-48dc-b554-c444c6219dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average mIoU:4.167839020662217e-05\n",
            "Average mIoU per class:[0.00079189 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data Augmentation"
      ],
      "metadata": {
        "id": "2PmIHy0Of0Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform_aug1 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomApply([\n",
        "        transforms.Compose([\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.RandomResizedCrop(size=(720, 1280), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "        ])\n",
        "    ], p=0.5),\n",
        "    transforms.Resize((720, 1280), interpolation=transforms.InterpolationMode.NEAREST),  # To make sure that the output is (720,1280) even if we do not apply augmentation\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_transform_aug2 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomApply([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.5),\n",
        "    transforms.Resize((720, 1280), interpolation=Image.Resampling.NEAREST),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_transform_aug3 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))], p=0.5),\n",
        "    transforms.Resize((720,1280), interpolation=Image.Resampling.NEAREST),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_lbl_transform = transforms.Compose([\n",
        "    transforms.Resize((720,1280), interpolation=Image.Resampling.NEAREST),\n",
        "])"
      ],
      "metadata": {
        "id": "2vBsnDs4gQZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((512,1024), interpolation=Image.Resampling.NEAREST),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_lbl_transform = transforms.Compose([\n",
        "    transforms.Resize((512,1024), interpolation=Image.Resampling.NEAREST),\n",
        "])"
      ],
      "metadata": {
        "id": "JKU-GiEvg98D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset1 = GTA5('/content/drive/MyDrive/MLDL_project/datasets/GTA5', transform=[train_transform_aug1,train_lbl_transform]) # datasets\\\\GTA5\n",
        "train_dataset2 = GTA5('/content/drive/MyDrive/MLDL_project/datasets/GTA5', transform=[train_transform_aug2,train_lbl_transform]) # datasets\\\\GTA5\n",
        "train_dataset3 = GTA5('/content/drive/MyDrive/MLDL_project/datasets/GTA5', transform=[train_transform_aug3,train_lbl_transform]) # datasets\\\\GTA5\n",
        "val_dataset = CityscapesDataset('/content/drive/MyDrive/MLDL_project/datasets/Cityspaces', transform=[val_transform,val_lbl_transform]) # datasets\\\\Cityscapes\\\\Cityspaces\n",
        "\n",
        "train_dataloader1 = DataLoader(train_dataset1, batch_size=4, shuffle=True, num_workers = 8) #16\n",
        "train_dataloader2 = DataLoader(train_dataset2, batch_size=4, shuffle=True, num_workers = 8) #16\n",
        "train_dataloader3 = DataLoader(train_dataset3, batch_size=4, shuffle=True, num_workers = 8) #16\n",
        "val_dataloader = DataLoader(val_dataset, split='val', batch_size=4, shuffle=True, num_workers = 8) #16"
      ],
      "metadata": {
        "id": "Z5VgaG3mgGS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "_pI9z-9EtQ7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and initial learning rate\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
        "initial_lr = 2.5e-2/4"
      ],
      "metadata": {
        "id": "zs3oyEZxuaG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "aug1"
      ],
      "metadata": {
        "id": "diRvkIQ7sOrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bisenet_aug1 = BiSeNet(num_classes = 19, context_path='resnet18').to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer_aug1 = torch.optim.SGD(bisenet_aug1.parameters(), lr=initial_lr, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# Start the timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "epochs = 5 #50\n",
        "\n",
        "miou_train_list_aug1 = []\n",
        "miou_test_list_aug1 = []\n",
        "miou_per_class_train_list_aug1 = []\n",
        "miou_per_class_test_list_aug1 = []\n",
        "\n",
        "for ep in range(epochs):\n",
        "\n",
        "    train_loss, train_miou, train_miou_per_class = train(bisenet_aug1, optimizer_aug1, train_dataloader1, loss_fn)\n",
        "    test_loss, test_miou, test_miou_per_class = test(bisenet_aug1, val_dataloader, loss_fn)\n",
        "\n",
        "    ## Inserting learning rate decay\n",
        "    curr_lr = poly_lr_scheduler(optimizer = optimizer_aug1, init_lr = initial_lr, iter = ep, lr_decay_iter=1, max_iter=epochs, power=0.9)\n",
        "\n",
        "    miou_train_list_aug1.append(train_miou)\n",
        "    miou_test_list_aug1.append(test_miou)\n",
        "    miou_per_class_train_list_aug1.append(train_miou_per_class)\n",
        "    miou_per_class_test_list_aug1.append(test_miou_per_class)\n",
        "\n",
        "    print(f\"Current learning rate: {curr_lr}\")\n",
        "    print(f\"Test loss: {test_loss}\")\n",
        "    print(f\"Train mIoU: {train_miou}\")\n",
        "    print(f\"Test mIoU: {test_miou}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "xgb-PhoMsMbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "aug2"
      ],
      "metadata": {
        "id": "twj5h9PHsQBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bisenet_aug2 = BiSeNet(num_classes = 19, context_path='resnet18').to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer_aug2 = torch.optim.SGD(bisenet_aug2.parameters(), lr=initial_lr, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# Start the timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "epochs = 5 #50\n",
        "\n",
        "miou_train_list_aug2 = []\n",
        "miou_test_list_aug2 = []\n",
        "miou_per_class_train_list_aug2 = []\n",
        "miou_per_class_test_list_aug2 = []\n",
        "\n",
        "for ep in range(epochs):\n",
        "\n",
        "    train_loss, train_miou, train_miou_per_class = train(bisenet_aug2, optimizer_aug2, train_dataloader1, loss_fn)\n",
        "    test_loss, test_miou, test_miou_per_class = test(bisenet_aug2, val_dataloader, loss_fn)\n",
        "\n",
        "    ## Inserting learning rate decay\n",
        "    curr_lr = poly_lr_scheduler(optimizer = optimizer_aug2, init_lr = initial_lr, iter = ep, lr_decay_iter=1, max_iter=epochs, power=0.9)\n",
        "\n",
        "    miou_train_list_aug2.append(train_miou)\n",
        "    miou_test_list_aug2.append(test_miou)\n",
        "    miou_per_class_train_list_aug2.append(train_miou_per_class)\n",
        "    miou_per_class_test_list_aug2.append(test_miou_per_class)\n",
        "\n",
        "    print(f\"Current learning rate: {curr_lr}\")\n",
        "    print(f\"Test loss: {test_loss}\")\n",
        "    print(f\"Train mIoU: {train_miou}\")\n",
        "    print(f\"Test mIoU: {test_miou}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "E4kOEu4TsQiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "aug3"
      ],
      "metadata": {
        "id": "PvH_Y3N_sQ8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bisenet_aug3 = BiSeNet(num_classes = 19, context_path='resnet18').to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer_aug3 = torch.optim.SGD(bisenet_aug3.parameters(), lr=initial_lr, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# Start the timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "epochs = 5 #50\n",
        "\n",
        "miou_train_list_aug3 = []\n",
        "miou_test_list_aug3 = []\n",
        "miou_per_class_train_list_aug3 = []\n",
        "miou_per_class_test_list_aug3 = []\n",
        "\n",
        "for ep in range(epochs):\n",
        "\n",
        "    train_loss, train_miou, train_miou_per_class = train(bisenet_aug3, optimizer_aug3, train_dataloader1, loss_fn)\n",
        "    test_loss, test_miou, test_miou_per_class = test(bisenet_aug3, val_dataloader, loss_fn)\n",
        "\n",
        "    ## Inserting learning rate decay\n",
        "    curr_lr = poly_lr_scheduler(optimizer = optimizer_aug3, init_lr = initial_lr, iter = ep, lr_decay_iter=1, max_iter=epochs, power=0.9)\n",
        "\n",
        "    miou_train_list_aug3.append(train_miou)\n",
        "    miou_test_list_aug3.append(test_miou)\n",
        "    miou_per_class_train_list_aug3.append(train_miou_per_class)\n",
        "    miou_per_class_test_list_aug3.append(test_miou_per_class)\n",
        "\n",
        "    print(f\"Current learning rate: {curr_lr}\")\n",
        "    print(f\"Test loss: {test_loss}\")\n",
        "    print(f\"Train mIoU: {train_miou}\")\n",
        "    print(f\"Test mIoU: {test_miou}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "mw2qpx3ZsRWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}